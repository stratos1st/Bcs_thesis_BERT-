{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Greek_bert_random_numbers.ipynb","provenance":[],"collapsed_sections":["4uHTL519iZVZ","nUbFqZMTYFOy","A89dNcfGViGB","CL6zGRvng0cb","t5SEw2hUfv5G","gMPeoqpOf7p3","oujVuInLieHc","_Ig-lwAAkh0w"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d30877a391b8408da96d64c32b7a02f2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f7cc17ee12594d8f9602b11eb4d6dbcf","IPY_MODEL_9b09a949ef0040ef80ffea36d0b96adb"],"layout":"IPY_MODEL_59db2ab4d1de43b596e337694f3413d2"}},"f7cc17ee12594d8f9602b11eb4d6dbcf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"split=test:  63%","description_tooltip":null,"layout":"IPY_MODEL_3cb0151dd5654c5daaa0657c7d1f0b37","max":1189,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fbe266f84c54843b50b53ddd8f4a166","value":746}},"9b09a949ef0040ef80ffea36d0b96adb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d6b62bfa25a4b569242c87510574765","placeholder":"​","style":"IPY_MODEL_f6784833b1614c3a9663375383f40343","value":" 746/1189 [03:42&lt;02:14,  3.28it/s]"}},"59db2ab4d1de43b596e337694f3413d2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3cb0151dd5654c5daaa0657c7d1f0b37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fbe266f84c54843b50b53ddd8f4a166":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"4d6b62bfa25a4b569242c87510574765":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f6784833b1614c3a9663375383f40343":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","metadata":{"id":"EEDBycDQOHOY","executionInfo":{"status":"ok","timestamp":1627031699689,"user_tz":-180,"elapsed":7,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}}},"source":["from argparse import Namespace\n","args = Namespace(\n","  # Data and path information\n","  dataset_path=\"/content/drive/MyDrive/thesis/dataset/\",\n","  model_state_file=\"model_subject_random_numbers.pth\", \n","  save_dir=\"/content/drive/MyDrive/thesis/models/\", # save models here\n","  no_classes = 2285, # subject 2285, chapter 389, volume 47\n","  class_name = 'subject',\n","  # Training hyper parameters\n","  seed=1338,\n","  num_epochs=15,\n","  early_stopping_criteria=2,\n","  learning_rate=0.00002,\n","  batch_size=8,\n","  # Runtime options\n","  cuda=True,\n","  reload_from_files=False, # to continue training from checkpoint or evaluate a model\n","  reload_name=\"model_chapter_8.pth\",\n","  expand_filepaths_to_save_dir=True,\n","  run_training=False #if false it will run only on the test set\n",")"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4uHTL519iZVZ"},"source":["###tmp"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9N1VlTthMldd","executionInfo":{"elapsed":42711,"status":"ok","timestamp":1620640503012,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"},"user_tz":-180},"outputId":"321a8aa2-06b6-482a-eade-ef49b0a4d802"},"source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rqkz0Hg1O2Up","executionInfo":{"status":"ok","timestamp":1627032234113,"user_tz":-180,"elapsed":529956,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"1fc6a9f5-4adb-4df0-999a-01ebde76ac42"},"source":["# from collections import Counter\n","import string\n","import re\n","import random\n","\n","import numpy as np\n","import pandas as pd\n","import pickle\n","from tqdm.notebook import tqdm\n","\n","!pip install torch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader\n","print('Loading torchmetrics lib...')\n","# # https://torchmetrics.readthedocs.io/en/latest/index.html\n","# !pip install torchmetrics\n","# from torchmetrics import MetricCollection, Accuracy, Precision, Recall, F1\n","from sklearn.metrics import f1_score, recall_score, precision_score\n","\n","# reading json files\n","import json\n","from os import listdir\n","from os.path import isfile, join\n","import os\n","\n","# huggingface lib bert\n","print('Loading transformers lib...')\n","!pip install transformers\n","from transformers import AutoTokenizer, AutoModel"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting torch\n","  Downloading torch-1.9.0-cp38-cp38-manylinux1_x86_64.whl (831.4 MB)\n","\u001b[K     |████████████████████████████████| 831.4 MB 52 kB/s \n","\u001b[?25hCollecting typing-extensions\n","  Downloading typing_extensions-3.10.0.0-py3-none-any.whl (26 kB)\n","Installing collected packages: typing-extensions, torch\n","\u001b[33m  WARNING: The scripts convert-caffe2-to-onnx and convert-onnx-to-caffe2 are installed in '/home/stratos/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","Successfully installed torch-1.9.0 typing-extensions-3.10.0.0\n","Loading torchmetrics lib...\n","Loading transformers lib...\n","Collecting transformers\n","  Downloading transformers-4.9.0-py3-none-any.whl (2.6 MB)\n","\u001b[K     |████████████████████████████████| 2.6 MB 237 kB/s \n","\u001b[?25hCollecting filelock\n","  Downloading filelock-3.0.12-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: requests in /usr/lib/python3/dist-packages (from transformers) (2.22.0)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 2.1 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in ./.local/lib/python3.8/site-packages (from transformers) (21.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.3.1)\n","Requirement already satisfied: numpy>=1.17 in ./.local/lib/python3.8/site-packages (from transformers) (1.21.1)\n","Requirement already satisfied: tqdm>=4.27 in ./.local/lib/python3.8/site-packages (from transformers) (4.61.2)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 1.5 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in ./.local/lib/python3.8/site-packages (from transformers) (2021.7.6)\n","Collecting huggingface-hub==0.0.12\n","  Downloading huggingface_hub-0.0.12-py3-none-any.whl (37 kB)\n","Requirement already satisfied: pyparsing>=2.0.2 in ./.local/lib/python3.8/site-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: joblib in ./.local/lib/python3.8/site-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: click in /usr/lib/python3/dist-packages (from sacremoses->transformers) (7.0)\n","Requirement already satisfied: six in /usr/lib/python3/dist-packages (from sacremoses->transformers) (1.14.0)\n","Requirement already satisfied: typing-extensions in ./.local/lib/python3.8/site-packages (from huggingface-hub==0.0.12->transformers) (3.10.0.0)\n","Installing collected packages: filelock, tokenizers, sacremoses, huggingface-hub, transformers\n","\u001b[33m  WARNING: The script sacremoses is installed in '/home/stratos/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","\u001b[33m  WARNING: The script huggingface-cli is installed in '/home/stratos/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","\u001b[33m  WARNING: The script transformers-cli is installed in '/home/stratos/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\n","Successfully installed filelock-3.0.12 huggingface-hub-0.0.12 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.9.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VlFvFQUvOAcZ","executionInfo":{"status":"ok","timestamp":1627032234904,"user_tz":-180,"elapsed":55,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"e1b0e6fd-e186-410d-db06-ccc347e0bb38"},"source":["# Check CUDA and gpu available\n","if not torch.cuda.is_available():\n","  args.cuda = False\n","args.device = torch.device(\"cuda\" if args.cuda else \"cpu\")\n","print(\"Using CUDA: {}\".format(args.cuda))\n","if args.cuda:\n","  print(\"GPU: {}\".format(torch.cuda.get_device_name(0)))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using CUDA: False\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"nUbFqZMTYFOy"},"source":["### Utils"]},{"cell_type":"code","metadata":{"id":"x5SVi3W8YFUb"},"source":["# sets the seed everywhere for reprodusable results\n","def set_seed_everywhere(seed, cuda):\n","  np.random.seed(seed)\n","  torch.manual_seed(seed)\n","  torch.cuda.manual_seed_all(seed)\n","  random.seed(seed)\n","  # !!!! may need to add hugingface init seed\n","  if cuda:\n","    torch.cuda.manual_seed_all(seed)\n","\n","# creates non existing directories\n","def handle_dirs(dirpath):\n","  if not os.path.exists(dirpath):\n","    os.makedirs(dirpath)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A89dNcfGViGB"},"source":["### The Vectorizer"]},{"cell_type":"code","metadata":{"id":"8jG3SJWzViOn"},"source":["class LegalVectorizer(object):\n","  \"\"\" The Vectorizer\"\"\"\n","  def __init__(self):\n","    print('Loading BERT tokenizer...')\n","    self.tokenizer = AutoTokenizer.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\", \n","                                                    model_max_length=512, use_fast=True)\n","\n","  def vectorize(self, text):\n","    \"\"\"\n","    Args:\n","        text (list of str):\n","    Returns:\n","        dictionary: \"vector\" is a tensor with a list of encoded text paded to max_len, ready for import to BERT\n","                    \"mask\" is a tensor with a list of masks ready for import to BERT\n","    \"\"\"\n","    encoded_dict = self.tokenizer(\n","                    text,                      \n","                    add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                    padding = True, # pad to longest in batch\n","                    truncation = True, # truncates sentenses to 512, max bert length\n","                    # padding = 'max_length',\n","                    # max_length = 512,           # Pad\n","                    return_attention_mask = True,   # Construct attn. masks.\n","                    return_tensors = 'pt',     # Return pytorch tensors.\n","                )\n","    return {\"vector\" : encoded_dict['input_ids'],\n","            \"mask\" : encoded_dict['attention_mask']}\n","\n","  def get_pad_tocken(self):\n","    return self.tokenizer.pad_token_id\n","\n","  def to_words(self, vector, remove_pads=True):\n","    \"\"\"\n","    Args:\n","        vector (tensor): list of vectors to decode\n","        remove_pads : remove pad \n","    Returns:\n","        list of lists of words coresponding to the vector values\n","    \"\"\"\n","    if remove_pads:\n","      ans=[self.tokenizer.convert_ids_to_tokens(v[v.nonzero()]) for v in vector]\n","    else:\n","      ans=[self.tokenizer.convert_ids_to_tokens(v) for v in vector]\n","    return ans\n","      "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CL6zGRvng0cb"},"source":["###The dataset"]},{"cell_type":"code","metadata":{"id":"9OVA8YQTg0QD"},"source":["class LegalDataset(Dataset):\n","  def __init__(self):\n","    print(\"loading validation set...\")\n","    self.val_df = pd.read_pickle(args.dataset_path + \"dev.pkl\")\n","    self.validation_size = len(self.val_df)\n","    print(\"loading training set...\")\n","    self.train_df = pd.read_pickle(args.dataset_path + \"train.pkl\")\n","    self.train_size = len(self.train_df)\n","    print(\"loading test set...\")\n","    self.test_df = pd.read_pickle(args.dataset_path + \"test.pkl\")\n","    self.test_size = len(self.test_df)\n","    # check the dataset size (dataset has extra files when uploaded to google drive. it mekes copies ex. \"123 (1).json\")\n","    if self.val_df.shape[0]!=9511 or self.train_df.shape[0]!=28536 or self.test_df.shape[0]!=9516:\n","      print(self.val_df.shape[0])\n","      print(self.train_df.shape[0])\n","      print(self.test_df.shape[0])\n","      print(\"!! ERROR dataset size !!\")\n","      exit()\n","\n","    print(\"Processing dataset...\")\n","    # replace class names with 0...n numbers\n","    class_namess = pd.concat([ self.val_df[args.class_name],\n","                             self.train_df[args.class_name],\n","                             self.test_df[args.class_name] ]).unique()\n","    self.class_names = dict(zip(class_namess, range(len(class_namess))))\n","    self.val_df[args.class_name] = self.val_df[args.class_name].replace(self.class_names)\n","    self.train_df[args.class_name] = self.train_df[args.class_name].replace(self.class_names)\n","    self.test_df[args.class_name] = self.test_df[args.class_name].replace(self.class_names)\n","    # delete usless stuff\n","    self.val_df = self.val_df.drop(['title', 'type', 'year', 'law_id', 'leg_uri'], axis=1)\n","    self.train_df = self.train_df.drop(['title', 'type', 'year', 'law_id', 'leg_uri'], axis=1)\n","    self.test_df = self.test_df.drop(['title', 'type', 'year', 'law_id', 'leg_uri'], axis=1)\n","\n","    def random_digits(y):\n","      while True:\n","        start = random.choice(string.digits)\n","        if int(start) != 0:\n","          break\n","      return start+''.join(random.choice(string.digits) for x in range(len(y.group())-1))\n","\n","    text = []\n","    for _, row in self.val_df.iterrows():\n","      tmp = row['header'] + ' ' + row['articles']\n","      text.append(re.sub(r\"\\d+\", random_digits, tmp))\n","    self.val_df['header'] = text\n","\n","    text = []\n","    for _, row in self.train_df.iterrows():\n","      tmp = row['header'] + ' ' + row['articles']\n","      text.append(re.sub(r\"\\d+\", random_digits, tmp))\n","    self.train_df['header'] = text\n","\n","    text = []\n","    for _, row in self.test_df.iterrows():\n","      tmp = row['header'] + ' ' + row['articles']\n","      text.append(re.sub(r\"\\d+\", random_digits, tmp))\n","    self.test_df['header'] = text\n","\n","    self._vectorizer = LegalVectorizer()\n","    self._lookup_dict = {'train': (self.train_df, self.train_size),\n","                          'val': (self.val_df, self.validation_size),\n","                          'test': (self.test_df, self.test_size)}\n","\n","    self.set_split('train')\n","\n","    print(\"Calculating frequences...\")\n","    self.class_counts = []\n","    for v in range(args.no_classes):\n","      tmp = self.train_df[self.train_df[args.class_name]==v][args.class_name].count()\n","      if tmp>0:\n","        self.class_counts.append(tmp)\n","      else:\n","        self.class_counts.append(0)\n","    # self.class_weights = 10000.0 / torch.tensor(self.class_counts, dtype=torch.float32) \n","\n","  def get_vectorizer(self):\n","    \"\"\" returns the vectorizer \"\"\"\n","    return self._vectorizer\n","\n","  def set_split(self, split):\n","    \"\"\" selects the splits in the dataset using _lookup_dict \"\"\"\n","    # self._target_split = split\n","    self._target_df, self._target_size = self._lookup_dict[split]\n","\n","  def __len__(self):\n","    return self._target_size\n","\n","  def __getitem__(self, index):\n","    \"\"\"the primary entry point method for PyTorch datasets\n","    Args:\n","        index (int): the index to the data point \n","    Returns:\n","        a dictionary holding the data point's. text is NOT vectorized yet.\n","    \"\"\"\n","    row = self._target_df.iloc[index]\n","    target = row[args.class_name]\n","    text = row['header']\n","\n","    return {'target': target,\n","            'text' : text}\n","      \n","  def get_num_batches(self, batch_size):\n","    \"\"\"Given a batch size, return the number of batches in the dataset\n","    Args:\n","      batch_size (int)\n","    Returns:\n","      number of batches in the dataset\n","    \"\"\"\n","    return len(self) // batch_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t5SEw2hUfv5G"},"source":["###Dataloader"]},{"cell_type":"code","metadata":{"id":"-q4zXTEffv_V"},"source":["# this is a cool way to save some time in training. Havent done it yet, may effect accuracy\n","# http://mccormickml.com/2020/07/29/smart-batching-tutorial\n","def generate_batches(dataset, batch_size, shuffle=True,\n","                     drop_last=True, device=\"cpu\"): \n","  dataloader = DataLoader(dataset=dataset, batch_size=batch_size,\n","                          shuffle=shuffle, drop_last=drop_last)\n","\n","  for data_dict in dataloader:\n","    #vectorize batch text\n","    tmp = dataset.get_vectorizer().vectorize(data_dict['text'])\n","    mask, vector = tmp['mask'], tmp['vector']\n","    data_dict['vector'] = vector\n","    data_dict['mask'] = mask\n","    del data_dict['text']\n","\n","    out_data_dict = {}\n","    for name, tensor in data_dict.items():\n","        out_data_dict[name] = data_dict[name].to(device)\n","    yield out_data_dict"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gMPeoqpOf7p3"},"source":["###The Model"]},{"cell_type":"code","metadata":{"id":"3GJCeb23f7wa"},"source":["class LegalClassifier(nn.Module):\n","  \"\"\" greek-Bert model with an extra linear layer, which takes \n","      the cls tocken as input, for classification \"\"\"\n","  def __init__(self, no_classes):\n","    \"\"\"\n","    Args:\n","        no_classes (int): the size of the linear layer\n","    \"\"\"\n","    super(LegalClassifier, self).__init__()\n","    print(\"Loading greek-Bert...\")\n","    self.bert = AutoModel.from_pretrained(\"nlpaueb/bert-base-greek-uncased-v1\")\n","    self.dropout = nn.Dropout(0.1)\n","    self.fc = nn.Linear(768,no_classes)\n","    # for param in self.bert.parameters():\n","    #   param.requires_grad = False\n","\n","  def forward(self, input, mask, apply_softmax=False):\n","    \"\"\"The forward pass of the classifier\n","    Args:\n","        input (torch.Tensor): an input data tensor.\n","        mask (torch.Tensor): the coresponding masks for BERT\n","        apply_softmax (bool): whether or not to apply soflmax to the output layer\n","    Returns:\n","        the resulting tensor. tensor.shape should be (batch, output_dim)\n","    \"\"\"\n","    output = self.bert(input_ids=input, attention_mask=mask, \n","                       output_attentions=False, output_hidden_states=False)\n","    x = output.pooler_output # coresponds to CLS token\n","    x = self.dropout(x) \n","    x = self.fc(x)\n","    if apply_softmax:\n","        x = F.softmax(x, dim=1)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oujVuInLieHc"},"source":["###helper functions"]},{"cell_type":"code","metadata":{"id":"wEbV1KG-ieOs"},"source":["def make_train_state(args):\n","  return {'stop_early': False,\n","          'early_stopping_step': 0,\n","          'early_stopping_best_val': 1e8,\n","          'learning_rate': args.learning_rate,\n","          'epoch_index': 0,\n","          'train_loss': [],\n","          'train_acc': [],\n","          'val_loss': [],\n","          'val_acc': [],\n","          'test_loss': -1,\n","          'test_acc': -1,\n","          'model_filename': args.model_state_file}\n","\n","def update_train_state(args, model, train_state):\n","  \"\"\"Handle the training state updates.\n","\n","  Components:\n","    - Early Stopping: Prevent overfitting.\n","    - Model Checkpoint: Model is saved if the model is better\n","\n","  :param args: main arguments\n","  :param model: model to train\n","  :param train_state: a dictionary representing the training state values\n","  :returns:\n","      a new train_state\n","  \"\"\"\n","  # Save one model at least\n","  if train_state['epoch_index'] == 0:\n","    torch.save(model.state_dict(), train_state['model_filename'])\n","    train_state['stop_early'] = False\n","\n","  # Save model if performance improved\n","  elif train_state['epoch_index'] >= 1:\n","    loss_tm1, loss_t = train_state['val_loss'][-2:]\n","\n","    # If loss worsened\n","    if loss_t >= train_state['early_stopping_best_val']:\n","      # Update step\n","      train_state['early_stopping_step'] += 1\n","    # Loss decreased\n","    else:\n","      # Save the best model\n","      if loss_t < train_state['early_stopping_best_val']:\n","          torch.save(model.state_dict(), train_state['model_filename'])\n","      # Reset early stopping step\n","      train_state['early_stopping_step'] = 0\n","\n","    # Stop early ?\n","    train_state['stop_early'] = \\\n","        train_state['early_stopping_step'] >= args.early_stopping_criteria\n","  return train_state\n","\n","def compute_accuracy(y_pred, y_target):\n","  _, y_pred_indices = y_pred.max(dim=1)\n","  n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n","  return n_correct / len(y_pred_indices) * 100"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_Ig-lwAAkh0w"},"source":["###Initializations"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zhbuS8_ZkiE4","executionInfo":{"elapsed":98424,"status":"ok","timestamp":1620640558772,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"},"user_tz":-180},"outputId":"5dab0398-f489-44af-d6cb-7f383327eeb3"},"source":["if args.expand_filepaths_to_save_dir:\n","  args.model_state_file = os.path.join(args.save_dir, args.model_state_file)\n","  args.reload_name = os.path.join(args.save_dir, args.reload_name)\n","  print(\"Expanded filepaths: \")\n","  print(\"\\t{}\".format(args.model_state_file))\n","  print(\"\\t{}\".format(args.reload_name))\n","\n","# Set seed for reproducibility\n","set_seed_everywhere(args.seed, args.cuda)\n","\n","# handle dirs\n","# handle_dirs(args.save_dir)\n","\n","dataset = LegalDataset()\n","vectorizer = dataset.get_vectorizer()\n","classifier = LegalClassifier(no_classes=args.no_classes)\n","\n","if args.reload_from_files:\n","    # training from a checkpoint\n","    print(\"Reloading previous model!\")\n","    classifier.load_state_dict(torch.load(args.reload_name))\n","else:\n","    print(\"Creating fresh!\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Expanded filepaths: \n","\t/content/drive/MyDrive/thesis/models/model_subject_random_numbers.pth\n","\t/content/drive/MyDrive/thesis/models/model_chapter_8.pth\n","loading validation set...\n","loading training set...\n","loading test set...\n","Processing dataset...\n","Loading BERT tokenizer...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"13a6162b63114e149df52bf3ada6c324","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=459.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c0176ba9d0b84faa9c4e89fdeed5c683","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=529930.0, style=ProgressStyle(descripti…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"076d69e446484e62af5e4d321f8601e0","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=112.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"545402fcd76a4ac6873369013e968c61","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=2.0, style=ProgressStyle(description_wi…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Calculating frequences...\n","Loading greek-Bert...\n"],"name":"stdout"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"bffd75b6c0044d41a65f633c8440c0c6","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='Downloading', max=454248854.0, style=ProgressStyle(descri…"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n","Creating fresh!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"f1h5wBjHw_Gs"},"source":["###Training Loop"]},{"cell_type":"code","metadata":{"id":"lMdq50aOw_Mr"},"source":["if args.run_training:\n","  classifier = classifier.to(args.device)\n","  # dataset.class_weights = dataset.class_weights.to(args.device)\n","\n","  loss_func = nn.CrossEntropyLoss()#weight=dataset.class_weights)\n","  optimizer = optim.Adam(classifier.parameters(), lr=args.learning_rate)\n","  scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer, \n","                                                  mode='min', factor=0.5, patience=1)\n","\n","  train_state = make_train_state(args)\n","\n","  epoch_bar = tqdm(desc='training routine', \n","                            total=args.num_epochs,\n","                            position=0)\n","\n","  dataset.set_split('train')\n","  train_bar = tqdm(desc='split=train',\n","                            total=dataset.get_num_batches(args.batch_size), \n","                            position=1, \n","                            leave=True)\n","  dataset.set_split('val')\n","  val_bar = tqdm(desc='split=val',\n","                          total=dataset.get_num_batches(args.batch_size), \n","                          position=1, \n","                          leave=True)\n","\n","  try:\n","    for epoch_index in range(args.num_epochs):\n","      train_state['epoch_index'] = epoch_index\n","\n","      # Iterate over training dataset\n","\n","      # setup: batch generator, set loss and acc to 0, set train mode on\n","\n","      dataset.set_split('train')\n","      batch_generator = generate_batches(dataset, \n","                                          batch_size=args.batch_size, \n","                                          device=args.device)\n","      running_loss = 0.0\n","      running_acc = 0.0\n","      classifier.train()\n","\n","      for batch_index, batch_dict in enumerate(batch_generator):\n","        # the training routine is these 5 steps:\n","\n","        # --------------------------------------\n","        # step 1. zero the gradients\n","        optimizer.zero_grad()\n","        # step 2. compute the output\n","        y_pred = classifier(batch_dict['vector'].squeeze(), batch_dict['mask'].squeeze())\n","        # step 3. compute the loss\n","        loss = loss_func(y_pred, batch_dict['target'])\n","        loss_t = loss.item()\n","        running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","        # step 4. use loss to produce gradients\n","        loss.backward()\n","\n","        # step 5. use optimizer to take gradient step\n","        optimizer.step()\n","        # -----------------------------------------\n","        # compute the accuracy\n","        acc_t = compute_accuracy(y_pred, batch_dict['target'])\n","        running_acc += (acc_t - running_acc) / (batch_index + 1)\n","\n","        # update bar\n","        train_bar.set_postfix(loss=running_loss, acc=running_acc, \n","                        epoch=epoch_index)\n","        train_bar.update()\n","\n","      train_state['train_loss'].append(running_loss)\n","      train_state['train_acc'].append(running_acc)\n","\n","      # Iterate over val dataset\n","\n","      # setup: batch generator, set loss and acc to 0; set eval mode on\n","      dataset.set_split('val')\n","      batch_generator = generate_batches(dataset, \n","                                          batch_size=args.batch_size, \n","                                          device=args.device)\n","      running_loss = 0.\n","      running_acc = 0.\n","      classifier.eval()\n","      with torch.no_grad():\n","        for batch_index, batch_dict in enumerate(batch_generator):\n","          # compute the output\n","          y_pred =  classifier(batch_dict['vector'].squeeze(), batch_dict['mask'].squeeze())\n","\n","          # compute the loss\n","          loss = loss_func(y_pred, batch_dict['target'])\n","          loss_t = loss.to(\"cpu\").item()\n","          running_loss += (loss_t - running_loss) / (batch_index + 1)\n","\n","          # compute the accuracy\n","          acc_t = compute_accuracy(y_pred, batch_dict['target'])\n","          running_acc += (acc_t - running_acc) / (batch_index + 1)\n","          val_bar.set_postfix(loss=running_loss, acc=running_acc, \n","                          epoch=epoch_index)\n","          \n","          val_bar.update()\n","\n","      train_state['val_loss'].append(running_loss)\n","      train_state['val_acc'].append(running_acc)\n","\n","      train_state = update_train_state(args=args, model=classifier,\n","                                        train_state=train_state)\n","\n","      scheduler.step(train_state['val_loss'][-1])\n","\n","      if train_state['stop_early']:\n","        break\n","\n","      train_bar.n = 0\n","      val_bar.n = 0\n","      epoch_bar.update()\n","  except KeyboardInterrupt:\n","      print(\"Exiting loop\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qOXfsB2Y69Bd"},"source":["### Testing"]},{"cell_type":"code","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["d30877a391b8408da96d64c32b7a02f2","f7cc17ee12594d8f9602b11eb4d6dbcf","9b09a949ef0040ef80ffea36d0b96adb","59db2ab4d1de43b596e337694f3413d2","3cb0151dd5654c5daaa0657c7d1f0b37","2fbe266f84c54843b50b53ddd8f4a166","4d6b62bfa25a4b569242c87510574765","f6784833b1614c3a9663375383f40343"]},"id":"2ENl_kjXBEki","outputId":"16d21d34-1b98-4ab8-9ece-ea8bf05424e9"},"source":["# run test set and save results\n","\n","preds = []\n","correct = []\n","\n","classifier.load_state_dict(torch.load(args.model_state_file))\n","classifier = classifier.to(args.device)\n","# dataset.class_weights = dataset.class_weights.to(args.device)\n","\n","dataset.set_split('test')\n","batch_generator = generate_batches(dataset, \n","                                   batch_size=args.batch_size, \n","                                   device=args.device)\n","\n","test_bar = tqdm(desc='split=test',\n","                          total=dataset.get_num_batches(args.batch_size), \n","                          position=1, \n","                          leave=True)\n","\n","classifier.eval()\n","with torch.no_grad():\n","  for batch_index, batch_dict in enumerate(batch_generator):\n","    # compute output\n","    y_pred =  classifier(batch_dict['vector'].squeeze(), batch_dict['mask'].squeeze(), apply_softmax=True)\n","    #save output\n","    preds += y_pred\n","    correct += batch_dict['target']\n","    test_bar.update()\n","\n","predictions = {'predictions' : preds,\n","              'correct' : correct}"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d30877a391b8408da96d64c32b7a02f2","version_major":2,"version_minor":0},"text/plain":["HBox(children=(FloatProgress(value=0.0, description='split=test', max=1189.0, style=ProgressStyle(description_…"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"aKIwkAS6UK1H"},"source":["# tensors to cpu and find predicted class\n","tmp = []\n","for item in tqdm(predictions['predictions'], desc='predictions to cpu'):\n","  item = item.cpu()\n","  tmp.append(item.tolist().index(max(item)))\n","predictions['predictions'] = tmp\n","predictions['correct'] = torch.tensor(predictions['correct'], device = 'cpu').tolist()\n","\n","frequent = {'predictions' : [],\n","            'correct' : []}\n","few_shot = {'predictions' : [],\n","            'correct' : []}\n","\n","for pred, target in zip(predictions['predictions'], predictions['correct']):\n","  if dataset.class_counts[target] < 10 and dataset.class_counts[target] != 0:\n","    few_shot['predictions'].append(pred)\n","    few_shot['correct'].append(target)\n","  elif dataset.class_counts[target] > 10:\n","    frequent['predictions'].append(pred)\n","    frequent['correct'].append(target)\n","\n","for group, dict_ in zip(['All     \\t','Frequent\\t','Fewshot \\t'], [predictions, frequent, few_shot]):\n","  if 'All' in group:\n","    l = list(set(dict_['correct']) | set(dict_['predictions']))\n","  else:\n","    l = list(set(dict_['correct']))\n","  f1 = round(f1_score(dict_['correct'], dict_['predictions'], average='micro', labels=l)*100,2)\n","  rec = round(recall_score(dict_['correct'], dict_['predictions'], average='micro', labels=l)*100,2)\n","  prec = round(precision_score(dict_['correct'], dict_['predictions'], average='micro', labels=l)*100,2)\n","  print(group+'\\t f1: '+str(f1)+'\\t recall: '+str(rec)+'\\t precision: '+str(prec))"],"execution_count":null,"outputs":[]}]}