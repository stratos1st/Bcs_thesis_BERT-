{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"crete_APT_input_files.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNHJxUQIlIIqnNvZVFKpWZp"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9ea5106543bc4bb0986c779dc9b30c8c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_3b1561cbd6784e649c4b8a1f68d16dc4","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_c3c59e9c52f64ae18024dce006902a98","IPY_MODEL_074b6b07c7614a91a10291761d67f9ca"]}},"3b1561cbd6784e649c4b8a1f68d16dc4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c3c59e9c52f64ae18024dce006902a98":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_6a439a0ebcd942baa2d3a3e469d84e1d","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":28536,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":28536,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ca3ec23580e54b7fb5e42fc4fb774a32"}},"074b6b07c7614a91a10291761d67f9ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_bf350dc1bdbc4b09b24e958c0e065a2a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 28536/28536 [01:25&lt;00:00, 334.89it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_366b705276a746ae8766842716af4793"}},"6a439a0ebcd942baa2d3a3e469d84e1d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"ca3ec23580e54b7fb5e42fc4fb774a32":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bf350dc1bdbc4b09b24e958c0e065a2a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"366b705276a746ae8766842716af4793":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"Nt8P6lOWcYpo"},"source":["create greek bert ckpt file. convert h5 file from [here](https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1/tree/main)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6qoP3ccd9J7","executionInfo":{"status":"ok","timestamp":1619874079236,"user_tz":-180,"elapsed":1798,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"5f310523-763d-49a4-890e-a46cc12086c7"},"source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDz8n8MZd-In","executionInfo":{"status":"ok","timestamp":1619874080097,"user_tz":-180,"elapsed":2619,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"55188e6c-b99b-4f53-965a-0099e91ff6bf"},"source":["import pandas as pd\n","from tqdm.notebook import tqdm\n","from nltk import tokenize\n","import nltk\n","nltk.download('punkt')\n","from string import digits\n","from statistics import median"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LgTMhaYZckrc"},"source":["###make txt\n","Create a huge txt file from the raptarchis training data using the folowing format from [here](https://github.com/google-research/bert/blob/master/create_pretraining_data.py)\n","\n","Input file format:\n","  (1) One sentence per line. These should ideally be actual sentences, not\n","  entire paragraphs or arbitrary spans of text. (Because we use the\n","  sentence boundaries for the \"next sentence prediction\" task).\n","  (2) Blank lines between documents. Document boundaries are needed so\n","  that the \"next sentence prediction\" task doesn't span between documents."]},{"cell_type":"code","metadata":{"id":"8UcExwuvckHa","colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["9ea5106543bc4bb0986c779dc9b30c8c","3b1561cbd6784e649c4b8a1f68d16dc4","c3c59e9c52f64ae18024dce006902a98","074b6b07c7614a91a10291761d67f9ca","6a439a0ebcd942baa2d3a3e469d84e1d","ca3ec23580e54b7fb5e42fc4fb774a32","bf350dc1bdbc4b09b24e958c0e065a2a","366b705276a746ae8766842716af4793"]},"executionInfo":{"status":"ok","timestamp":1619874166789,"user_tz":-180,"elapsed":89281,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"af27e2e2-04d9-4175-d4b4-5c4e9dcb6e33"},"source":["train_df = pd.read_pickle(\"/content/drive/MyDrive/thesis/dataset/train.pkl\")\n","remove_digits = str.maketrans('', '', digits)\n","try:\n","  with open(\"/content/drive/MyDrive/thesis/dataset/raptarchis_for_pretraining.txt\",\"w+\") as f:\n","    for _, row in tqdm(train_df.iterrows(), total=len(train_df)):\n","      tmp = row['header'] + \" \" + row['articles']\n","      text = tmp.translate(remove_digits)\n","      text = text.replace('\\n', ' ').replace('\\r', '')\n","      sentenses = tokenize.sent_tokenize(text, language='greek')\n","      sent_len = 0\n","      cont = 0\n","      for i, _ in enumerate(sentenses):\n","        if cont > 0:\n","          cont -= 1\n","          continue\n","        sent = sentenses[i]\n","        sent_len += len(sent)\n","        if sent_len > 20 and len(sent) > 5:\n","          f.write(sent)\n","          j = i+1\n","          while j < len(sentenses) and len(sentenses[j]) <= 5:\n","            f.write(sentenses[j] + ' ')\n","            j += 1\n","          cont = j-i-1\n","          end = True\n","          sent_len = 0\n","          f.write('\\n')\n","        else:\n","          end = False\n","          f.write(sent + ' ')\n","      f.write('\\n') if end else f.write('\\n\\n')\n","except KeyboardInterrupt:\n","  print(\"Exiting loop\")\n","    # time.sleep(1)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9ea5106543bc4bb0986c779dc9b30c8c","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=28536.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZQDukXD5SNX","executionInfo":{"status":"ok","timestamp":1619874169171,"user_tz":-180,"elapsed":91656,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"00999b37-b3d0-4e22-83f0-0696fcb6e268"},"source":["sum_sent = 0\n","sum_sent_list = []\n","no_lines = 0\n","no_lines_list = []\n","no_doc = 0\n","lines_per_doc = 0\n","one_sent_docs = 0\n","with open(\"/content/drive/MyDrive/thesis/dataset/raptarchis_for_pretraining.txt\",\"r\") as f:\n","  for line in f:\n","    if line == '\\n':\n","      no_doc += 1\n","      no_lines_list.append(lines_per_doc)\n","      if lines_per_doc == 1:\n","        one_sent_docs += 1\n","      lines_per_doc = 0\n","      continue\n","    sum_sent += len(line)\n","    sum_sent_list.append(len(line))\n","    no_lines += 1\n","    lines_per_doc += 1\n","print(f\"mean number of words per sentense {sum_sent/no_lines}\")\n","print(f\"mean number of sentenses per document {no_lines/no_doc}\")\n","print(f\"median number of words per sentense {median(sum_sent_list)}\")\n","print(f\"median number of sentenses per document {median(no_lines_list)}\")\n","print(f\"min number of words per sentense {min(sum_sent_list)}\")\n","print(f\"min number of sentenses per document {min(no_lines_list)}\")\n","print(f\"max number of words per sentense {max(sum_sent_list)}\")\n","print(f\"max number of sentenses per document {max(no_lines_list)}\")\n","print(f\"number of documents with only one sentese {one_sent_docs}\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["mean number of words per sentense 136.67636948876392\n","mean number of sentenses per document 29.98293383795907\n","median number of words per sentense 100\n","median number of sentenses per document 7.0\n","min number of words per sentense 8\n","min number of sentenses per document 1\n","max number of words per sentense 6623\n","max number of sentenses per document 8090\n","number of documents with only one sentese 352\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o2oX02d_7oYx","executionInfo":{"status":"ok","timestamp":1619874169179,"user_tz":-180,"elapsed":91655,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"4a25d0b1-c986-4b24-8ae1-bfa660b66ecd"},"source":["with open(\"/content/drive/MyDrive/thesis/dataset/raptarchis_for_pretraining.txt\",\"r\") as f:\n","  for i, line in enumerate(f):\n","    print(line)\n","    if i>40:\n","      break\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":[". ΑΠΟΦΑΣΗ ΥΠΟΥΡΓΩΝ ΓΕΩΡΓΙΑΣ ΚΑΙ ΕΜΠΟΡΙΟΥ Αριθ.\n","\n","Α/ της  Μαρτ./ Απρ. (ΦΕΚ Β΄ ) Σύσταση συγκρότηση Ειδικού Υπηρεσιακού Συμβουλίου του Οργανισμού Αγοράς Αθηνών (Ν.Π.Δ.Δ.). \n","\n","\n","\n",". ΠΡΟΕΔΡΙΚΟ ΔΙΑΤΑΓΜΑ υπ’ αριθ.\n","\n","της / Νοεμ. (ΦΕΚ Α΄ ) Αυτόματες σταθμικές μηχανές διαλογής ελέγχου και κατατάξεως, σε συμμόρφωση προς την Οδηγία //ΕΟΚ του Συμβουλίου των Ευρωπαϊκών Κοινοτήτων της ης Δεκ. .\n","\n","΄Εχοντας υπόψη: α)Το άρθρ.\n","\n","του Νόμ. / «περί κυρώσεως της Συνθήκης Προσχωρήσεως της Ελλάδος εις την Ευρωπαϊκήν Οικονομικήν Κοινότητα και την Ευρωπαϊκή Κοινότητα Ατομικής Ενεργείας ως και της συμφωνίας περί προσχωρήσεως της Ελλάδος εις την Ευρωπαϊκή Κοινότητα ΄Ανθρακος και Χάλυβος».\n","\n","β)Τα άρθρ.,  και  της Πράξεως «περί των όρων προσχωρήσεως της Ελληνικής Δημοκρατίας και των προσαρμογών των Συνθηκών» η οποία κυρώθηκε με το Νόμ.\n","\n","/ (ΦΕΚ /τ.Α΄/..). γ)Τις διατάξεις της παρ.\n","\n","του άρθρ. του Νόμ. / «εφαρμογή του κοινοτικού δικαίου» (ΦΕΚ  τ.Α΄/..).\n","\n","δ)Τις διατάξεις του άρθρ.\n","\n","του Νόμ. / «περί εκπροσωπήσεως της Ελλάδος στις Ευρωπαϊκές Κοινότητες, ιδρύσεως Διπλωματικών και Προξενικών Αρχών και ρυθμίσεως άλλων συναφών  οργανικών θεμάτων» (ΦΕΚ /τ.Α΄/..) σε συνδυασμό με την παρ.\n","\n","του άρθρ. του Π.Δ. / «Ανακατανομή των αρμοδιοτήτων των Υπουργείων» (ΦΕΚ//τ.Α΄/..).\n","\n","ε)Τις / και / γνωμοδοτήσεις του Συμβουλίου της Επικρατείας, με πρόταση των Υπουργών Εθνικής Οικονομίας και Εμπορίου, αποφασίζουμε:  ΄Αρθρ..-Το Π.Δ/μα εκδίδεται με σκοπό την συμμόρφωση της Ελληνικής Νομοθεσίας προς τις διατάξεις της //ΕΟΚ Οδηγίας του Συμβουλίου των Ευρωπαϊκών Κοινοτήτων και του παραρτήματός της, η οποία έχει δημοσιευθεί στην Ελληνική Γλώσσα, στην Επίσημη Εφημερίδα των Ευρωπαϊκών Κοινοτήτων (Ειδική ΄Εκδοση της .., κατηγορία  Βιομηχανική Πολιτική, τόμος , σελ. ).\n","\n","΄Αρθρ..-Το Π.Δ/μα αυτό εφαρμόζεται στις αυτόματες σταθμικές μηχανές διαλογής ελέγχου και κατατάξεως, που κατανέμουν ένα σύνολο αντικειμένων σε δύο ή περισσότερα υποσύνολα, σε συνάρτηση με τη μάζα αυτών των αντικειμένων.\n","\n","Τα όργανα αυτά ορίζονται στο σημείο Ι του παραρτήματος της Οδηγίας //ΕΟΚ.\n","\n","΄Αρθρ..-.Οι αυτόματες σταθμικές μηχανές διαλογής ελέγχου και κατατάξεως, που μπορούν να λάβουν τα σήματα και τις ενδείξεις ΕΟΚ, καθορίζονται στα κεφαλ.\n","\n","ΙΙ και ΙΙΙ του παραρτήματος της Οδηγίας //ΕΟΚ.\n","\n",".Οι μηχανές αυτές αποτελούν αντικείμενο εγκρίσεως προτύπου ΕΟΚ και υπόκεινται στον Αρχικό ΄Ελεγχο ΕΟΚ, σύμφωνα με τις διατάξεις των κεφαλ.\n","\n","IV και V του παραρτήματος της Οδηγίας //ΕΟΚ και τις διατάξεις του Π.Δ.\n","\n","/ «περί οργάνων μετρήσεως και μεθόδων μετρολογικού ελέγχου».\n","\n","΄Αρθρ..-Δεν μπορεί να απαγορευθεί ή να περιορισθεί η θέση σε κυκλοφορία στην αγορά και η χρήση των αυτομάτων σταθμικών μηχανών διαλογής ελέγχου και κατατάξεως, που φέρουν το σήμα εγκρίσεως προτύπου ΕΟΚ και τη σφραγίδα Αρχικού Ελέγχου ΕΟΚ, για λόγους που αφορούν τις μετρολογικές τους ιδιότητες.\n","\n","΄Αρθρ..-Η αρμόδια Υπηρεσία του Υπουργείου Εμπορίου ενημερώνει αμέσως την Επιτροπή των Ευρωπαϊκών Κοινοτήτων, για τις αναγκαίες νομοθετικές, κανονιστικές και διοικητικές διατάξεις που έχουν ληφθεί και γνωστοποιεί σ’ αυτή το κείμενο των ουσιωδών διατάξεων του Ελληνικού Δικαίου, οι οποίες έχουν θεσπισθεί στον τομέα που διέπεται από την Οδηγία //ΕΟΚ.\n","\n","΄Αρθρ..-Η ισχύς του παρόντος αρχίζει από την δημοσίευσή του στην Εφημερίδα της Κυβερνήσεως.\n","\n","Κάθε άλλη διάταξη που αντίκειται στο παρόν καταργείται.\n","\n","(Αντί για τη σελ. ,(δ) Σελ. ,(ε) Τεύχος -Σελ.  Μέτρα και Σταθμά .Β.β.\n","\n","\n","\n",". ΑΝΑΓΚΑΣΤΙΚΟΣ ΝΟΜΟΣ της / Οκτ.\n","\n","Περί τροποποιήσεως διατάξεων των άρθρ.\n","\n","και  του ν. . Άρθρ.-.-Τροποποιούνται οι παραγρ.- του άρθρου  και  του άρθρ.\n","\n","ν.  (ανωτ. αριθ.). Άρθρ..-(Άνευ αντικειμένου μετά το άρθρ.Α.Ν. \n","\n","/, κατωτ. αριθ. ). Άρθρ..-Η ισχύς των διατάξεων  των άρθρ.\n","\n","και  του παρόντος άρχεται από  Οκτ.. \n","\n","\n","\n",". ΝΟΜΟΘΕΤ. ΔΙΑΤΑΓΜΑ  της / Δεκ.  Περί κανονισμού του τρόπου εισπράξεως του εγγείου φόρου επί του επτανησιακού σταφιδοκάρπου κατά την δια θαλάσσης μεταφοράν προς τοπικήν κατανάλωσιν.\n","\n","Ο ως άνω φόρος κατηργήθη υπό του άρθρ.παρ. εδάφ. \n","\n","δ' του Νόμ. (. Βα. ). \n","\n","\n","\n",". ΑΠΟΦΑΣΗ ΥΠΟΥΡΓΟΥ ΕΘΝΙΚΗΣ ΑΜΥΝΑΣ.Αριθ. \n","\n","Φ./ της / Ιουλ. (ΦΕΚ Β' ).\n","\n","Νοσηλεία Αναπήρων Πολέμου σε Νοσοκομεία των Ενόπλων Δυνάμεων.\n","\n","Στα Νοσοκομεία των Ενόπλων Δυνάμεων (Στρατού–Ναυτικού–Αεροπορίας) αναλόγως προελεύσεως δύνανται να νοσηλεύονται ανάπηροι πολέμου για παθήσεις που έχουν σχέση με το πολεμικό τραύμα τους.\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"cbfgISHG_1SH"},"source":["###make .ckpt\n","convert .h5 to .ckpt \n",".h5 from [here](https://huggingface.co/nlpaueb/bert-base-greek-uncased-v1/tree/main)"]},{"cell_type":"code","metadata":{"id":"yzuU2Z0lZcLt","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1620210573273,"user_tz":-180,"elapsed":318208,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"5358072c-9cc1-4a85-c60f-430940c80a09"},"source":["!pip install --upgrade tensorflow\n","import tensorflow as tf\n","from keras.models import load_model\n","\n","saver = tf.train.Checkpoint()\n","model = load_model('/home/stratos/Downloads/Greek_BERT.h5', compile=False)\n","sess = tf.compat.v1.keras.backend.get_session()\n","\n","save_path = saver.save(\"/home/stratos/Downloads/Greek_BERT.ckpt\")"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting tensorflow\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/70/dc/e8c5e7983866fa4ef3fd619faa35f660b95b01a2ab62b3884f038ccab542/tensorflow-2.4.1-cp37-cp37m-manylinux2010_x86_64.whl (394.3MB)\n","\u001b[K     |████████████████████████████████| 394.3MB 14kB/s \n","\u001b[?25hCollecting grpcio~=1.32.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/54/1c8be62beafe7fb1548d2968e518ca040556b46b0275399d4f3186c56d79/grpcio-1.32.0-cp37-cp37m-manylinux2014_x86_64.whl (3.8MB)\n","\u001b[K     |████████████████████████████████| 3.8MB 1.9MB/s \n","\u001b[?25hCollecting keras-preprocessing~=1.1.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/4c/7c3275a01e12ef9368a892926ab932b33bb13d55794881e3573482b378a7/Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42kB)\n","\u001b[K     |████████████████████████████████| 51kB 102kB/s \n","\u001b[?25hCollecting flatbuffers~=1.12.0\n","  Downloading https://files.pythonhosted.org/packages/eb/26/712e578c5f14e26ae3314c39a1bdc4eb2ec2f4ddc89b708cf8e0a0d20423/flatbuffers-1.12-py2.py3-none-any.whl\n","Collecting absl-py~=0.10\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/c9/ef0fae29182d7a867d203f0eff8296b60da92098cc41db33a434f4be84bf/absl_py-0.12.0-py3-none-any.whl (129kB)\n","\u001b[K     |████████████████████████████████| 133kB 1.6MB/s \n","\u001b[?25hCollecting tensorboard~=2.4\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/44/f5/7feea02a3fb54d5db827ac4b822a7ba8933826b36de21880518250b8733a/tensorboard-2.5.0-py3-none-any.whl (6.0MB)\n","\u001b[K     |████████████████████████████████| 6.0MB 57kB/s \n","\u001b[?25hCollecting astunparse~=1.6.3\n","  Using cached https://files.pythonhosted.org/packages/2b/03/13dde6512ad7b4557eb792fbcf0c653af6076b81e5941d36ec61f7ce6028/astunparse-1.6.3-py2.py3-none-any.whl\n","Collecting numpy~=1.19.2\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/08/d6/a6aaa29fea945bc6c61d11f6e0697b325ff7446de5ffd62c2fa02f627048/numpy-1.19.5-cp37-cp37m-manylinux2010_x86_64.whl (14.8MB)\n","\u001b[K     |████████████████████████████████| 14.8MB 160kB/s \n","\u001b[?25hCollecting gast==0.3.3\n","  Using cached https://files.pythonhosted.org/packages/d6/84/759f5dd23fec8ba71952d97bcc7e2c9d7d63bdc582421f3cd4be845f0c98/gast-0.3.3-py2.py3-none-any.whl\n","Collecting tensorflow-estimator<2.5.0,>=2.4.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/74/7e/622d9849abf3afb81e482ffc170758742e392ee129ce1540611199a59237/tensorflow_estimator-2.4.0-py2.py3-none-any.whl (462kB)\n","\u001b[K     |████████████████████████████████| 471kB 1.6MB/s \n","\u001b[?25hCollecting wheel~=0.35\n","  Downloading https://files.pythonhosted.org/packages/65/63/39d04c74222770ed1589c0eaba06c05891801219272420b40311cd60c880/wheel-0.36.2-py2.py3-none-any.whl\n","Collecting wrapt~=1.12.1\n","  Using cached https://files.pythonhosted.org/packages/82/f7/e43cefbe88c5fd371f4cf0cf5eb3feccd07515af9fd6cf7dbf1d1793a797/wrapt-1.12.1.tar.gz\n","Requirement already satisfied, skipping upgrade: termcolor~=1.1.0 in ./.local/lib/python3.7/site-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied, skipping upgrade: h5py~=2.10.0 in ./anaconda3/lib/python3.7/site-packages (from tensorflow) (2.10.0)\n","Collecting typing-extensions~=3.7.4\n","  Downloading https://files.pythonhosted.org/packages/60/7a/e881b5abb54db0e6e671ab088d079c57ce54e8a01a3ca443f561ccadb37e/typing_extensions-3.7.4.3-py3-none-any.whl\n","Collecting six~=1.15.0\n","  Downloading https://files.pythonhosted.org/packages/ee/ff/48bde5c0f013094d729fe4b0316ba2a24774b3ff1c52d924a8a4cb04078a/six-1.15.0-py2.py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in ./.local/lib/python3.7/site-packages (from tensorflow) (3.10.0)\n","Collecting opt-einsum~=3.3.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/19/404708a7e54ad2798907210462fd950c3442ea51acc8790f3da48d2bee8b/opt_einsum-3.3.0-py3-none-any.whl (65kB)\n","\u001b[K     |████████████████████████████████| 71kB 309kB/s \n","\u001b[?25hCollecting google-pasta~=0.2\n","  Using cached https://files.pythonhosted.org/packages/a3/de/c648ef6835192e6e2cc03f40b19eeda4382c49b5bafb43d88b931c4c74ac/google_pasta-0.2.0-py3-none-any.whl\n","Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in ./anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.4.1)\n","Requirement already satisfied, skipping upgrade: setuptools>=41.0.0 in ./anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (42.0.2.post20191203)\n","Collecting tensorboard-plugin-wit>=1.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/c1/499e600ba0c618b451cd9c425ae1c177249940a2086316552fee7d86c954/tensorboard_plugin_wit-1.8.0-py3-none-any.whl (781kB)\n","\u001b[K     |████████████████████████████████| 788kB 2.0MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: markdown>=2.6.8 in ./.local/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (3.1.1)\n","Collecting tensorboard-data-server<0.7.0,>=0.6.0\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/63/d92b4bc44261b7396558c054f78acf71468b5628bcb14cdaeb2504ea80d3/tensorboard_data_server-0.6.0-py3-none-manylinux2010_x86_64.whl (3.9MB)\n","\u001b[K     |████████████████████████████████| 3.9MB 394kB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in ./anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (1.10.0)\n","Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in ./anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (0.14.1)\n","Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in ./anaconda3/lib/python3.7/site-packages (from tensorboard~=2.4->tensorflow) (2.21.0)\n","Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in ./anaconda3/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (1.0.0)\n","Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in ./anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.2.2)\n","Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in ./anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.0.0)\n","Requirement already satisfied, skipping upgrade: rsa<4.1,>=3.1.4 in ./anaconda3/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (4.0)\n","Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in ./anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (1.24.2)\n","Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in ./anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2.7)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in ./anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (3.0.4)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in ./anaconda3/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard~=2.4->tensorflow) (2018.8.24)\n","Requirement already satisfied, skipping upgrade: oauthlib>=0.6.2 in ./anaconda3/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.4->tensorflow) (2.1.0)\n","Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.1 in ./anaconda3/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard~=2.4->tensorflow) (0.4.4)\n","Building wheels for collected packages: wrapt\n","  Building wheel for wrapt (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Created wheel for wrapt: filename=wrapt-1.12.1-cp37-cp37m-linux_x86_64.whl size=77205 sha256=d5205014351f0df53cb268348921bf74350d4e3c1063130364b774808c6956d5\n","  Stored in directory: /home/stratos/.cache/pip/wheels/b1/c2/ed/d62208260edbd3fa7156545c00ef966f45f2063d0a84f8208a\n","Successfully built wrapt\n","\u001b[31mERROR: tensorflow-gpu 1.13.1 has requirement tensorboard<1.14.0,>=1.13.0, but you'll have tensorboard 2.5.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tensorflow-gpu 1.13.1 has requirement tensorflow-estimator<1.14.0rc0,>=1.13.0, but you'll have tensorflow-estimator 2.4.0 which is incompatible.\u001b[0m\n","\u001b[31mERROR: tf-nightly 2.1.0.dev20191219 has requirement gast==0.2.2, but you'll have gast 0.3.3 which is incompatible.\u001b[0m\n","\u001b[31mERROR: jupyterlab-server 1.0.0 has requirement jsonschema>=3.0.1, but you'll have jsonschema 2.6.0 which is incompatible.\u001b[0m\n","Installing collected packages: six, grpcio, numpy, keras-preprocessing, flatbuffers, absl-py, tensorboard-plugin-wit, tensorboard-data-server, wheel, tensorboard, astunparse, gast, tensorflow-estimator, wrapt, typing-extensions, opt-einsum, google-pasta, tensorflow\n","  Found existing installation: six 1.11.0\n","    Uninstalling six-1.11.0:\n","      Successfully uninstalled six-1.11.0\n","  Found existing installation: grpcio 1.24.1\n","    Uninstalling grpcio-1.24.1:\n","      Successfully uninstalled grpcio-1.24.1\n","  Found existing installation: numpy 1.20.2\n","    Uninstalling numpy-1.20.2:\n","      Successfully uninstalled numpy-1.20.2\n","  Found existing installation: Keras-Preprocessing 1.1.0\n","    Uninstalling Keras-Preprocessing-1.1.0:\n","      Successfully uninstalled Keras-Preprocessing-1.1.0\n","  Found existing installation: absl-py 0.8.1\n","    Uninstalling absl-py-0.8.1:\n","      Successfully uninstalled absl-py-0.8.1\n","  Found existing installation: wheel 0.33.6\n","    Uninstalling wheel-0.33.6:\n","      Successfully uninstalled wheel-0.33.6\n","  Found existing installation: tensorboard 1.13.0\n","    Uninstalling tensorboard-1.13.0:\n","      Successfully uninstalled tensorboard-1.13.0\n","  Found existing installation: gast 0.2.2\n","    Uninstalling gast-0.2.2:\n","      Successfully uninstalled gast-0.2.2\n","  Found existing installation: tensorflow-estimator 1.13.0\n","    Uninstalling tensorflow-estimator-1.13.0:\n","      Successfully uninstalled tensorflow-estimator-1.13.0\n","  Found existing installation: wrapt 1.10.11\n","\u001b[31mERROR: Cannot uninstall 'wrapt'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"],"name":"stdout"},{"output_type":"error","ename":"ImportError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpywrap_tensorflow_internal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_mod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0m_pywrap_tensorflow_internal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m     \u001b[0;32mdel\u001b[0m \u001b[0mswig_import_helper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\u001b[0m in \u001b[0;36mswig_import_helper\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0m_mod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'_pywrap_tensorflow_internal'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/imp.py\u001b[0m in \u001b[0;36mload_module\u001b[0;34m(name, file, filename, details)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mload_dynamic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPKG_DIRECTORY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.7/imp.py\u001b[0m in \u001b[0;36mload_dynamic\u001b[0;34m(name, path, file)\u001b[0m\n\u001b[1;32m    341\u001b[0m             name=name, loader=loader, origin=path)\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: libcublas.so.10.0: cannot open shared object file: No such file or directory","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2eaebe01b630>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install --upgrade tensorflow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msaver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCheckpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# pylint: disable=g-bad-import-order\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m  \u001b[0;31m# pylint: disable=unused-import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Protocol buffers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msome\u001b[0m \u001b[0mcommon\u001b[0m \u001b[0mreasons\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0msolutions\u001b[0m\u001b[0;34m.\u001b[0m  \u001b[0mInclude\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mentire\u001b[0m \u001b[0mstack\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m above this error message when asking for help.\"\"\" % traceback.format_exc()\n\u001b[0;32m---> 74\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;31m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mImportError\u001b[0m: Traceback (most recent call last):\n  File \"/home/stratos/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow.py\", line 58, in <module>\n    from tensorflow.python.pywrap_tensorflow_internal import *\n  File \"/home/stratos/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 28, in <module>\n    _pywrap_tensorflow_internal = swig_import_helper()\n  File \"/home/stratos/.local/lib/python3.7/site-packages/tensorflow/python/pywrap_tensorflow_internal.py\", line 24, in swig_import_helper\n    _mod = imp.load_module('_pywrap_tensorflow_internal', fp, pathname, description)\n  File \"/home/stratos/anaconda3/lib/python3.7/imp.py\", line 242, in load_module\n    return load_dynamic(name, filename, file)\n  File \"/home/stratos/anaconda3/lib/python3.7/imp.py\", line 342, in load_dynamic\n    return _load(spec)\nImportError: libcublas.so.10.0: cannot open shared object file: No such file or directory\n\n\nFailed to load the native TensorFlow runtime.\n\nSee https://www.tensorflow.org/install/errors\n\nfor some common reasons and solutions.  Include the entire stack trace\nabove this error message when asking for help."]}]}]}