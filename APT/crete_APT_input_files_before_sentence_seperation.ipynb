{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"crete_APT_input_files_before_sentence_seperation.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNuiL3eE7RL/24Xk65jKz1E"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D6qoP3ccd9J7","executionInfo":{"status":"ok","timestamp":1623423151256,"user_tz":-180,"elapsed":258,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"e4870db0-ffa9-405a-c026-3a7ef8f5261a"},"source":["# mount drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eDz8n8MZd-In","executionInfo":{"status":"ok","timestamp":1623423151525,"user_tz":-180,"elapsed":10,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"711ff652-8d75-483e-886a-25193b73f039"},"source":["train_file = \"/content/drive/MyDrive/thesis/dataset/train.pkl\"\n","test_file = \"/content/drive/MyDrive/thesis/dataset/dev.pkl\"\n","output_train = \"/content/drive/MyDrive/thesis/dataset/raptarchis_for_pretraining_train.txt\"\n","output_test = \"/content/drive/MyDrive/thesis/dataset/raptarchis_for_pretraining_test.txt\"\n","\n","min_sent_len = 50\n","max_sent_len = 200\n","min_next_sent_len = 11\n","\n","import pandas as pd\n","from tqdm.notebook import tqdm\n","from nltk import tokenize\n","import nltk\n","nltk.download('punkt')\n","from string import digits\n","from statistics import median"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LgTMhaYZckrc"},"source":["###make txt\n","slpit to sentences. one per line\n","\n","<!-- Create a huge txt file from the raptarchis training data using the folowing format from [here](https://github.com/google-research/bert/blob/master/create_pretraining_data.py)\n","\n","Input file format:\n","  (1) One sentence per line. These should ideally be actual sentences, not\n","  entire paragraphs or arbitrary spans of text. (Because we use the\n","  sentence boundaries for the \"next sentence prediction\" task).\n","  (2) Blank lines between documents. Document boundaries are needed so\n","  that the \"next sentence prediction\" task doesn't span between documents. -->"]},{"cell_type":"markdown","metadata":{"id":"-9B_1gB7AbUp"},"source":["###funcs"]},{"cell_type":"code","metadata":{"id":"DiiqHAohAX7b"},"source":["def split_str(seq, chunk):\n","  lst = []\n","  if chunk <= len(seq):\n","    i=0\n","    for i, txt in enumerate(seq[chunk:]):\n","      if seq[chunk+i]==' ':\n","        break\n","    lst.extend([seq[:chunk+i]])\n","    lst.extend(split_str(seq[chunk+i:], chunk))\n","  elif seq:\n","    lst.extend([seq])\n","  return lst"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8UcExwuvckHa"},"source":["def create_txt(input_df, output_file):\n","  df = pd.read_pickle(input_df)\n","  # df = df.sample(frac = 1)\n","\n","  #first pass. deal with small sentences\n","  with open(output_file,\"w+\") as f:\n","    for _, row in tqdm(df.iterrows(), total=len(df), desc=\"1st_pass\"):\n","      tmp = row['header'] + \" \" + row['articles']\n","      text = tmp.replace('\\n', ' ').replace('\\r', '')\n","      sentenses = tokenize.sent_tokenize(text, language='greek')\n","      sent_len = 0\n","      cont = 0\n","      final = \"\"\n","      for i, _ in enumerate(sentenses):\n","        if cont > 0:\n","          cont -= 1\n","          continue\n","        sent = sentenses[i]\n","        final += sent + ' '\n","        sent_len += len(sent)\n","        if sent_len > min_sent_len:\n","          j = i+1\n","          while j < len(sentenses) and len(sentenses[j]) <= min_next_sent_len:\n","            final += sentenses[j] + ' '\n","            j += 1\n","            cont += 1\n","          end = True\n","          sent_len = 0\n","          final += '\\n'\n","        else:\n","          end = False\n","      f.write(final+'\\n') if end else f.write(final+'\\n\\n')\n","  #second pass. deal with large sentences\n","  with open(output_file,\"r\") as f:\n","    data = f.readlines()\n","  data2 = data\n","  for i, line in tqdm(enumerate(data), total=len(data), desc=\"2nd_pass\"):\n","    if len(line) > max_sent_len:\n","      how_many = int(len(line)/max_sent_len+1)\n","      how_many = int(len(line)/how_many)\n","      ans = split_str(line, how_many)\n","      if len(ans[-1])<=min_next_sent_len:\n","        ans[-2] = ans[-2]+ans[-1]\n","        del ans[-1]\n","      ans = [a+'\\n' for a in ans]\n","      ans[-1] = ans[-1].replace('\\n\\n\\n', '\\n\\n')  \n","      data2[i:i+1] = ans\n","  with open(output_file,\"w+\") as f:\n","    f.writelines(data2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mZQDukXD5SNX"},"source":["def print_stats(input_txt):\n","  sum_sent = 0\n","  sum_sent_list = []\n","  no_lines = 0\n","  no_lines_list = []\n","  no_doc = 0\n","  lines_per_doc = 0\n","  one_sent_docs = 0\n","  no_max_chars=0\n","  with open(input_txt,\"r\") as f:\n","    for line in f:\n","      if line == '\\n':\n","        if lines_per_doc == 0:\n","          continue\n","        no_doc += 1\n","        no_lines_list.append(lines_per_doc)\n","        if lines_per_doc == 1:\n","          one_sent_docs += 1\n","        lines_per_doc = 0\n","        continue\n","      sum_sent += len(line)\n","      sum_sent_list.append(len(line))\n","      no_lines += 1\n","      lines_per_doc += 1\n","      if len(line)>max_sent_len+2:\n","        no_max_chars += 1\n","  print(f\"total documents {no_doc}\")\n","  print(f\"mean number of chars per sentense {sum_sent/no_lines}\")\n","  print(f\"mean number of sentenses per document {no_lines/no_doc}\")\n","  print(f\"median number of chars per sentense {median(sum_sent_list)}\")\n","  print(f\"median number of sentenses per document {median(no_lines_list)}\")\n","  print(f\"min number of chars per sentense {min(sum_sent_list)}\")\n","  print(f\"min number of sentenses per document {min(no_lines_list)}\")\n","  print(f\"max number of chars per sentense {max(sum_sent_list)}\")\n","  print(f\"max number of sentenses per document {max(no_lines_list)}\")\n","  print(f\"number of documents with only one sentese {one_sent_docs}\")\n","  print(f\"number of sentenses with only min chars {sum_sent_list.count(min(sum_sent_list))}\")\n","  print(f\"number of sentenses with more than max chars {no_max_chars}\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PTKavC6jAhjK"},"source":["###results"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8e3uA62E_6Fl","executionInfo":{"status":"ok","timestamp":1623423152227,"user_tz":-180,"elapsed":707,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"5d70ccde-1f43-4bcb-c92f-cae17e31fdaa"},"source":["# create_txt(test_file, output_test)\n","print_stats(output_test)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total documents 75152\n","mean number of chars per sentense 126.80087526637121\n","mean number of sentenses per document 4.0712688950393865\n","median number of chars per sentense 128.0\n","median number of sentenses per document 3.0\n","min number of chars per sentense 14\n","min number of sentenses per document 1\n","max number of chars per sentense 507\n","max number of sentenses per document 199\n","number of documents with only one sentese 2149\n","number of sentenses with only min chars 63\n","number of sentenses with more than max chars 804\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UyvdgBkHtSGU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1623423154140,"user_tz":-180,"elapsed":1917,"user":{"displayName":"1strman","photoUrl":"","userId":"01784038301378029157"}},"outputId":"c40929a5-dbca-4466-ac6a-b091ea58e14f"},"source":["# create_txt(train_file, output_train)\n","print_stats(output_train)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["total documents 235244\n","mean number of chars per sentense 127.00422870589117\n","mean number of sentenses per document 4.086340140449916\n","median number of chars per sentense 128\n","median number of sentenses per document 3.0\n","min number of chars per sentense 14\n","min number of sentenses per document 1\n","max number of chars per sentense 331\n","max number of sentenses per document 217\n","number of documents with only one sentese 6248\n","number of sentenses with only min chars 137\n","number of sentenses with more than max chars 2437\n"],"name":"stdout"}]}]}